{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMF Demo\n",
    "\n",
    "This is the notebook for the WMF demo of Tumult Analytics.\n",
    "\n",
    "## Installation instructions\n",
    "\n",
    "First, follow the [installation instructions for Tumult Analytics](https://docs.tmlt.dev/analytics/latest/howto-guides/installation.html).\n",
    "\n",
    "Then, install two packages used to display the graphs in this notebook:\n",
    "\n",
    "```\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e09e20a-f77d-42d9-a548-1860fbe915e6",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82667,
     "status": "ok",
     "timestamp": 1698762696708,
     "user": {
      "displayName": "Amritha Pai",
      "userId": "04123723881848863103"
     },
     "user_tz": 240
    },
    "id": "R2ZCg05LMbMv",
    "outputId": "e42def02-4676-4cb4-8fa5-d0ab407ec65e"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, concat, expr\n",
    "from pyspark.sql.functions import abs as pyabs\n",
    "\n",
    "from tmlt.analytics import (\n",
    "    AddRowsWithID,\n",
    "    KeySet,\n",
    "    MaxGroupsPerID,\n",
    "    MaxRowsPerGroupPerID,\n",
    "    MaxRowsPerID,\n",
    "    PureDPBudget,\n",
    "    QueryBuilder,\n",
    "    RhoZCDPBudget,\n",
    "    Session,\n",
    ")\n",
    "from tmlt.core.utils.exact_number import ExactNumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6707cd51-99d4-49bd-bf20-d2f385eacecd",
     "showTitle": false,
     "title": ""
    },
    "id": "6yYQOD7uMbMw"
   },
   "source": [
    "\n",
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca7e684-bb9e-4073-93ed-93fb5731eb76",
     "showTitle": false,
     "title": ""
    },
    "id": "3iXZUVv3IZPB"
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "def epsilon_delta_to_rho_conversion(epsilon, delta):\n",
    "    exact_epsilon = ExactNumber.from_float(epsilon, round_up=True).expr\n",
    "    exact_delta = ExactNumber.from_float(delta, round_up=True).expr\n",
    "    rho = float(-2 * sp.sqrt(-sp.log(exact_delta)) * sp.sqrt(exact_epsilon - sp.log(exact_delta))\n",
    "        - 2 * sp.log(exact_delta)\n",
    "        + exact_epsilon)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b430cef-4af5-477f-ba1c-d25e552e7611",
     "showTitle": false,
     "title": ""
    },
    "id": "_lgDU5K2MbMy"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile(\n",
    "    \"https://gitlab.com/tumult-labs/demo-data/-/raw/main/demos/wmf/v1/wmf_countries.csv\"\n",
    ")\n",
    "spark.sparkContext.addFile(\n",
    "    \"https://gitlab.com/tumult-labs/demo-data/-/raw/main/demos/wmf/v1/wmf_input_250_k.csv\"\n",
    ")\n",
    "spark.sparkContext.addFile(\n",
    "    \"https://gitlab.com/tumult-labs/demo-data/-/raw/main/demos/wmf/v1/wmf_public_page_views.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9b79dd-2bbf-4708-b8fe-7755c9077dd8",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 2326,
     "status": "error",
     "timestamp": 1698762753999,
     "user": {
      "displayName": "Amritha Pai",
      "userId": "04123723881848863103"
     },
     "user_tz": 240
    },
    "id": "thP9rsD7MbMy",
    "outputId": "da11a015-4697-414c-bfde-118758bdaf2a"
   },
   "outputs": [],
   "source": [
    "wmf_df = spark.read.csv(\n",
    "    SparkFiles.get(\"wmf_input_250_k.csv\"),\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "grouping_column = concat(col(\"project\"), col(\"pageid\"), col(\"country\"))\n",
    "wmf_df = wmf_df.withColumn(\"grouping_column\", grouping_column)\n",
    "wmf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "345d7551-dcae-4b26-aa77-b57af23d427f",
     "showTitle": false,
     "title": ""
    },
    "id": "vwnRIwu4MbMy"
   },
   "source": [
    "\n",
    "#### Ground truth calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5838f398-eda5-49a6-b7cb-0f5ea623cecd",
     "showTitle": false,
     "title": ""
    },
    "id": "HEwzNpj_MbMz"
   },
   "outputs": [],
   "source": [
    "ground_truth_df = wmf_df.groupBy(\"project\", \"pageid\", \"country\").agg(\n",
    "    {\"actor_signature\": \"count\"}\n",
    ").withColumnRenamed(\"count(actor_signature)\", \"true_counts\").orderBy(\"true_counts\", ascending=False).alias(\"ground_truth\")\n",
    "\n",
    "ground_truth_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1cf4cf-b2b5-4849-b750-66bf21393fe5",
     "showTitle": false,
     "title": ""
    },
    "id": "J_XThvrAIZPD"
   },
   "source": [
    "\n",
    "#### Error calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed21bf5-eeef-488c-8708-196ca018c4aa",
     "showTitle": false,
     "title": ""
    },
    "id": "TfjFknmEMbM0"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import when, isnull, abs, round, lit, col, expr, greatest\n",
    "\n",
    "def calculate_median_relative_error(ground_truth_df, dp_df, join_columns, true_values, dp_values):\n",
    "  \"\"\"Calculates the median relative error between two columns in two joined DataFrames.\n",
    "\n",
    "  Args:\n",
    "    ground_truth_df (DataFrame): The first DataFrame to join.\n",
    "    dp_df (DataFrame): The second DataFrame to join.\n",
    "    join_columns (List[str]): The columns to join the DataFrames on.\n",
    "    true_values (str): The column in the first DataFrame to calculate the relative error for.\n",
    "    dp_values (str): The column in the second DataFrame to calculate the relative error for.\n",
    "  \"\"\"\n",
    "\n",
    "  combined_df = ground_truth_df.join(dp_df, on=join_columns, how=\"right\")\n",
    "  magic_percentile = expr('percentile_approx(relerror * 100, 0.5)')\n",
    "  mape = combined_df.withColumn(\"abserror\", col(true_values) - col(dp_values)).withColumn(\"relerror\", abs(col(\"abserror\")/greatest(col(true_values), lit(1)))).agg(magic_percentile.alias(\"median_perc_error\")).collect()[0]['median_perc_error']\n",
    "  return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375a475d-bfcc-4a40-ac0b-317bc2761cf5",
     "showTitle": false,
     "title": ""
    },
    "id": "KGSOWTneMbM0"
   },
   "outputs": [],
   "source": [
    "def spurious_rate(ground_truth_df, dp_df, join_columns, true_values, dp_values):\n",
    "  \"\"\"Calculates spurious rate between two columns in two joined DataFrames.\n",
    "\n",
    "  Args:\n",
    "    ground_truth_df (DataFrame): The first DataFrame to join.\n",
    "    dp_df (DataFrame): The second DataFrame to join.\n",
    "    join_columns (List[str]): The columns to join the DataFrames on.\n",
    "    true_values (str): The column in the first DataFrame to calculate the spurious rate for.\n",
    "    dp_values (str): The column in the second DataFrame to calculate the spurious rate for.\n",
    "  \"\"\"\n",
    "  combined_df = ground_truth_df.join(dp_df, join_columns, \"outer\")\n",
    "  total_published = dp_df.count()\n",
    "  if total_published == 0:\n",
    "    return float(\"nan\")\n",
    "  total_non_spurious_published = combined_df.dropna(\n",
    "    subset=[true_values, dp_values]\n",
    "  ).count()\n",
    "  total_spurious_published = total_published - total_non_spurious_published\n",
    "  return (total_spurious_published / total_published)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b3c758-e4fa-402f-a4b4-414195b2290d",
     "showTitle": false,
     "title": ""
    },
    "id": "F2jR3P2gIZPE"
   },
   "outputs": [],
   "source": [
    "def calculate_total_published_ratio(dp_df):\n",
    "  \"\"\"Calculates the total published ratio compared to hypothetical past release.\n",
    "\n",
    "  Args:\n",
    "    dp_df (DataFrame): Differentially private results DataFrame.\n",
    "  \"\"\"\n",
    "  return dp_df.count() / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b359847-3b06-4e8b-9cc8-c1ad2d8bf8ad",
     "showTitle": false,
     "title": ""
    },
    "id": "UXpVGUYlIZPE"
   },
   "source": [
    "\n",
    "#### Differential Privacy using Tumult Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b7102e-8095-4150-a216-cf268b1f2042",
     "showTitle": false,
     "title": ""
    },
    "id": "ghJgPyUqMbMz"
   },
   "source": [
    "\n",
    "#### First run using pureDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f863ee77-5752-45ec-9fae-07a318a22353",
     "showTitle": false,
     "title": ""
    },
    "id": "kOUE2KJ8MbMz"
   },
   "outputs": [],
   "source": [
    "EPSILON = 5\n",
    "\n",
    "# Levers (tunable)\n",
    "KEYSET_THRESHOLD = 10\n",
    "OUTPUT_THRESHOLD = 20\n",
    "ROWS_PER_ID = 10\n",
    "\n",
    "#  We will generate keysets that only include project x pageid combinations that are above a certain threshold.\n",
    "public_page_views_df = (\n",
    "    spark.read.csv(\n",
    "        SparkFiles.get(\"wmf_public_page_views.csv\"),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .where(col(\"page_views\") >= KEYSET_THRESHOLD)\n",
    "    .select(\"project\", \"pageid\")\n",
    ")\n",
    "countries_df = spark.read.csv(\n",
    "    SparkFiles.get(\"wmf_countries.csv\"),\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "keyset = KeySet.from_dataframe(public_page_views_df) *  KeySet.from_dataframe(countries_df)\n",
    "\n",
    "grouping_column = concat(col(\"project\"), col(\"pageid\"), col(\"country\"))\n",
    "new_keyset_df = keyset.dataframe().withColumn(\"grouping_column\", grouping_column)\n",
    "keyset = KeySet.from_dataframe(new_keyset_df)\n",
    "\n",
    "keyset.dataframe().show()\n",
    "\n",
    "\n",
    "puredp_sess = Session.from_dataframe(\n",
    "    privacy_budget=PureDPBudget(EPSILON),\n",
    "    source_id=\"wmf_synthetic_data\",\n",
    "    dataframe=wmf_df,\n",
    "    protected_change=AddRowsWithID(\"actor_signature\"),\n",
    ")\n",
    "\n",
    "query = (\n",
    "    QueryBuilder(\"wmf_synthetic_data\")\n",
    "    .enforce(MaxRowsPerID(ROWS_PER_ID))\n",
    "    .groupby(keyset)\n",
    "    .count()\n",
    ")\n",
    "\n",
    "dp_df = puredp_sess.evaluate(query, PureDPBudget(EPSILON))\n",
    "dp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4770fb5b-e92c-4023-80ea-2af57f002fcd",
     "showTitle": false,
     "title": ""
    },
    "id": "tlHUkilyIZPE"
   },
   "outputs": [],
   "source": [
    "filtered_dp_df = dp_df.where(f\"count >= {OUTPUT_THRESHOLD}\")\n",
    "mre = calculate_median_relative_error(ground_truth_df, filtered_dp_df, [\"project\", \"pageid\", \"country\"], \"true_counts\", \"count\")\n",
    "sr = spurious_rate(ground_truth_df, filtered_dp_df, [\"project\", \"pageid\", \"country\"], \"true_counts\", \"count\")\n",
    "total_published_ratio = calculate_total_published_ratio(filtered_dp_df)\n",
    "print(\"median_relative_error\",  mre, \"%\")\n",
    "print(\"spurious_rate\", sr, \"%\")\n",
    "print(\"total_published_ratio\", total_published_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8acab5b6-b281-417a-af18-ea18c227b480",
     "showTitle": false,
     "title": ""
    },
    "id": "NWTraW0DMbM0"
   },
   "source": [
    "\n",
    "#### Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e696f836-762b-4340-8e7d-89496539dc82",
     "showTitle": false,
     "title": ""
    },
    "id": "xLDNnbf0MbM0"
   },
   "source": [
    "\n",
    "#### Differentially Private answer using Tumult Analytics ZCDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe93e70-4f77-4d71-a579-b1128b5a5abc",
     "showTitle": false,
     "title": ""
    },
    "id": "AlLaQtSIMbM0"
   },
   "outputs": [],
   "source": [
    "EPSILON = 5\n",
    "DELTA = 1e-5\n",
    "ROWS_PER_GROUP_PER_ID = 1\n",
    "\n",
    "# Levers (tunable)\n",
    "KEYSET_THRESHOLD = 10\n",
    "OUTPUT_THRESHOLD = 20\n",
    "GROUPS_PER_ID = 10\n",
    "RHO = epsilon_delta_to_rho_conversion(EPSILON, DELTA)\n",
    "\n",
    "#  We will generate keysets that only include project x pageid combinations that are above a certain threshold.\n",
    "public_page_views_df = (\n",
    "    spark.read.csv(\n",
    "        SparkFiles.get(\"wmf_public_page_views.csv\"),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .where(col(\"page_views\") >= KEYSET_THRESHOLD)\n",
    "    .select(\"project\", \"pageid\")\n",
    ")\n",
    "countries_df = spark.read.csv(\n",
    "    SparkFiles.get(\"wmf_countries.csv\"),\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "keyset = KeySet.from_dataframe(public_page_views_df) *  KeySet.from_dataframe(countries_df)\n",
    "\n",
    "grouping_column = concat(col(\"project\"), col(\"pageid\"), col(\"country\"))\n",
    "new_keyset_df = keyset.dataframe().withColumn(\"grouping_column\", grouping_column)\n",
    "keyset = KeySet.from_dataframe(new_keyset_df)\n",
    "\n",
    "keyset.dataframe().show()\n",
    "\n",
    "zcdp_sess = Session.from_dataframe(\n",
    "    privacy_budget=RhoZCDPBudget(RHO),\n",
    "    source_id=\"wmf_synthetic_data\",\n",
    "    dataframe=wmf_df,\n",
    "    protected_change=AddRowsWithID(\"actor_signature\"),\n",
    ")\n",
    "\n",
    "query = (\n",
    "    QueryBuilder(\"wmf_synthetic_data\")\n",
    "    .enforce(MaxGroupsPerID(\"grouping_column\", GROUPS_PER_ID)) #20\n",
    "    .enforce(MaxRowsPerGroupPerID(\"grouping_column\", ROWS_PER_GROUP_PER_ID))\n",
    "    .groupby(keyset)\n",
    "    .count()\n",
    ")\n",
    "dp_df = zcdp_sess.evaluate(query, RhoZCDPBudget(RHO))\n",
    "dp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a18ab3-0ea4-4210-b03f-f0c7bdcdb3bb",
     "showTitle": false,
     "title": ""
    },
    "id": "JyAYRTLWIZPF"
   },
   "outputs": [],
   "source": [
    "filtered_dp_df = dp_df.where(f\"count >= {OUTPUT_THRESHOLD}\")\n",
    "mre = calculate_median_relative_error(ground_truth_df, filtered_dp_df, [\"project\", \"pageid\", \"country\"], \"true_counts\", \"count\")\n",
    "sr = spurious_rate(ground_truth_df, filtered_dp_df, [\"project\", \"pageid\", \"country\"], \"true_counts\", \"count\")\n",
    "total_published_ratio = calculate_total_published_ratio(filtered_dp_df)\n",
    "print(\"median_relative_error\",  mre, \"%\")\n",
    "print(\"spurious_rate\", sr, \"%\")\n",
    "print(\"total_published_ratio\", total_published_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6bf03d-d546-4147-a03b-ab2d8334d6ee",
     "showTitle": false,
     "title": ""
    },
    "id": "UKQUrh2gIZPF"
   },
   "source": [
    "#### Optimization - tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a174ec-3150-46e8-bfcb-e7b748006c45",
     "showTitle": false,
     "title": ""
    },
    "id": "ahChje-wIZPF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sympy as sp\n",
    "from tmlt.core.utils.parameters import calculate_noise_scale\n",
    "from tmlt.core.measures import RhoZCDP\n",
    "\n",
    "keyset_thresholds = [10]\n",
    "groups_per_ids = [10]\n",
    "\n",
    "def grid_search(epsilon_deltas, output_thresholds):\n",
    "    zcdp_sess = Session.from_dataframe(\n",
    "        privacy_budget=RhoZCDPBudget(float(\"inf\")),\n",
    "        source_id=\"wmf_synthetic_data\",\n",
    "        dataframe=wmf_df,\n",
    "        protected_change=AddRowsWithID(\"actor_signature\"),\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for epsilon, delta in epsilon_deltas:\n",
    "        rho = epsilon_delta_to_rho_conversion(epsilon, delta)\n",
    "\n",
    "        for keyset_threshold in keyset_thresholds:\n",
    "            public_page_views_df = (\n",
    "                spark.read.csv(\n",
    "                    SparkFiles.get(\"wmf_public_page_views.csv\"),\n",
    "                    header=True,\n",
    "                    inferSchema=True\n",
    "                )\n",
    "                .where(col(\"page_views\") >= KEYSET_THRESHOLD)\n",
    "                .select(\"project\", \"pageid\")\n",
    "            )\n",
    "            countries_df = spark.read.csv(\n",
    "                SparkFiles.get(\"wmf_countries.csv\"),\n",
    "                header=True,\n",
    "                inferSchema=True\n",
    "            )\n",
    "            keyset = KeySet.from_dataframe(public_page_views_df) *  KeySet.from_dataframe(countries_df)\n",
    "\n",
    "            grouping_column = concat(col(\"project\"), col(\"pageid\"), col(\"country\"))\n",
    "            new_keyset_df = keyset.dataframe().withColumn(\"grouping_column\", grouping_column)\n",
    "            keyset = KeySet.from_dataframe(new_keyset_df)\n",
    "\n",
    "            for groups_per_id in groups_per_ids:\n",
    "                query = (\n",
    "                    QueryBuilder(\"wmf_synthetic_data\")\n",
    "                    .enforce(MaxGroupsPerID(\"grouping_column\", groups_per_id))\n",
    "                    .enforce(MaxRowsPerGroupPerID(\"grouping_column\", ROWS_PER_GROUP_PER_ID))\n",
    "                    .groupby(keyset)\n",
    "                    .count()\n",
    "                )\n",
    "                dp_df = zcdp_sess.evaluate(query, RhoZCDPBudget(rho))\n",
    "\n",
    "                for output_threshold in output_thresholds:\n",
    "                    filtered_dp_df = dp_df.where(f\"count >= {output_threshold}\")\n",
    "                    mre = calculate_median_relative_error(ground_truth_df, filtered_dp_df, [\"project\", \"pageid\", \"country\"], \"true_counts\", \"count\")\n",
    "                    sr = spurious_rate(ground_truth_df, filtered_dp_df, [\"project\", \"pageid\", \"country\"], \"true_counts\", \"count\")\n",
    "                    total_published_ratio = calculate_total_published_ratio(filtered_dp_df)\n",
    "                    results.append({\n",
    "                        \"epsilon\": epsilon,\n",
    "                        \"rho\": rho,\n",
    "                        \"keyset_threshold\": keyset_threshold,\n",
    "                        \"output_threshold\": output_threshold,\n",
    "                        \"groups_per_id\": groups_per_id,\n",
    "                        \"median_relative_error\": mre,\n",
    "                        \"spurious_rate\": sr,\n",
    "                        \"total_published_ratio\": total_published_ratio,\n",
    "\n",
    "                    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cad8d2-c684-4c62-94a8-c5da478fced2",
     "showTitle": false,
     "title": ""
    },
    "id": "R1_sZ0IrIZPF"
   },
   "outputs": [],
   "source": [
    "# Tuning epsilon and output_threshold\n",
    "epsilon_deltas = [(5, 1e-5), (4, 1e-5), (3, 1e-5), (2, 1e-5), (1, 1e-5) ]\n",
    "output_thresholds = range(5, 85, 5)\n",
    "\n",
    "results_sdf = grid_search(epsilon_deltas, output_thresholds)\n",
    "display(results_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjpFOlNOIZPF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spark.sparkContext.addFile(\n",
    "    \"https://gitlab.com/tumult-labs/demo-data/-/raw/main/demos/wmf/v1/grid_report_2_new.csv\"\n",
    ")\n",
    "results_sdf = spark.read.csv(\n",
    "    SparkFiles.get(\"grid_report_2_new.csv\"),\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "display(results_sdf.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4308fb8-792b-486f-9909-0be54082509a",
     "showTitle": false,
     "title": ""
    },
    "id": "fM4dYY2MIZPF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results_sdf.toPandas())\n",
    "\n",
    "metrics = ['median_relative_error', 'spurious_rate', 'total_published_ratio']\n",
    "titles = ['Median Relative Error (%) with Max Groups Contributed = 10', 'Spurious Rate (%) with Max Groups Contributed = 10', 'Total Published Ratio with Max Groups Contributed = 10']\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[i]\n",
    "    heatmap_df = results_df.pivot(index=\"output_threshold\", columns=\"epsilon\", values=metric)\n",
    "    heatmap_df = heatmap_df.fillna(1)\n",
    "    heatmap_df = heatmap_df.round(2)\n",
    "    heatmap_df = heatmap_df.iloc[::-1]\n",
    "    if metric not in [\"spurious_rate\"]:\n",
    "        norm = mcolors.LogNorm()  # Logarithmic scale\n",
    "    else:\n",
    "        norm = None\n",
    "    sns.heatmap(heatmap_df, annot=True, fmt=\".2f\", cmap=\"Reds_r\", ax=ax, norm=norm, cbar=False)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Output Threshold\")\n",
    "    ax.set_title(title)\n",
    "    # ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4587dfa1-456e-4cb0-a75d-d81cad5ce72a",
     "showTitle": false,
     "title": ""
    },
    "id": "7lxcVXAfMbM1"
   },
   "source": [
    "\n",
    "#### Differentially Private answer using Tumult Analytics ZCDP and finalized param values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58300155-ff2e-45e2-bd19-78d1987845b1",
     "showTitle": false,
     "title": ""
    },
    "id": "7OYHRj9sMbM1"
   },
   "outputs": [],
   "source": [
    "EPSILON = 2\n",
    "DELTA = 1e-5\n",
    "ROWS_PER_GROUP_PER_ID = 1\n",
    "\n",
    "# Levers (tunable)\n",
    "KEYSET_THRESHOLD = 10\n",
    "OUTPUT_THRESHOLD = 40\n",
    "GROUPS_PER_ID = 10\n",
    "RHO = epsilon_delta_to_rho_conversion(EPSILON, DELTA)\n",
    "\n",
    "#  We will generate keysets that only include project x pageid combinations that are above a certain threshold.\n",
    "public_page_views_df = (\n",
    "    spark.read.csv(\n",
    "        SparkFiles.get(\"wmf_public_page_views.csv\"),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .where(col(\"page_views\") >= KEYSET_THRESHOLD)\n",
    "    .select(\"project\", \"pageid\")\n",
    ")\n",
    "countries_df = spark.read.csv(\n",
    "    SparkFiles.get(\"wmf_countries.csv\"),\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "keyset = KeySet.from_dataframe(public_page_views_df) *  KeySet.from_dataframe(countries_df)\n",
    "\n",
    "grouping_column = concat(col(\"project\"), col(\"pageid\"), col(\"country\"))\n",
    "new_keyset_df = keyset.dataframe().withColumn(\"grouping_column\", grouping_column)\n",
    "keyset = KeySet.from_dataframe(new_keyset_df)\n",
    "\n",
    "keyset.dataframe().show()\n",
    "\n",
    "zcdp_sess = Session.from_dataframe(\n",
    "    privacy_budget=RhoZCDPBudget(RHO),\n",
    "    source_id=\"wmf_synthetic_data\",\n",
    "    dataframe=wmf_df,\n",
    "    protected_change=AddRowsWithID(\"actor_signature\"),\n",
    ")\n",
    "\n",
    "query = (\n",
    "    QueryBuilder(\"wmf_synthetic_data\")\n",
    "    .enforce(MaxGroupsPerID(\"grouping_column\", GROUPS_PER_ID))\n",
    "    .enforce(MaxRowsPerGroupPerID(\"grouping_column\", ROWS_PER_GROUP_PER_ID))\n",
    "    .groupby(keyset)\n",
    "    .count()\n",
    ")\n",
    "\n",
    "dp_df = zcdp_sess.evaluate(query, RhoZCDPBudget(RHO))\n",
    "dp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iav8rRQWIZPG"
   },
   "outputs": [],
   "source": [
    "filtered_dp_df = dp_df.where(f\"count >= {OUTPUT_THRESHOLD}\")\n",
    "mre = calculate_median_relative_error(ground_truth_df, filtered_dp_df, [\"project\", \"pageid\", \"country\"], \"true_counts\", \"count\")\n",
    "sr = spurious_rate(ground_truth_df, filtered_dp_df, [\"project\", \"pageid\", \"country\"], \"true_counts\", \"count\")\n",
    "total_published_ratio = calculate_total_published_ratio(filtered_dp_df)\n",
    "print(\"median_relative_error\",  mre, \"%\")\n",
    "print(\"spurious_rate\", sr, \"%\")\n",
    "print(\"total_published_ratio\", total_published_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "Copyright Tumult Labs 2025"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wmf_demo_local_compatible",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
